{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "import time\n",
    "\n",
    "np.random.seed(1234) # 훈련과정에서 고정된 값을 위해\n",
    "def randomize(): np.random.seed(time.time()) # 최종적으로는 랜덤적으로 쓰기 위해"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "RND_MEAN = 0\n",
    "RND_STD = 0.0030\n",
    "# 정규 분포 난수값의 평균과 표준 편차 \n",
    "# => 가중치 파라미터 초기화에 이용\n",
    "\n",
    "LEARNING_RATE = 0.001 # 학습률!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def abalone_exec(epoch_count=10, mb_size=10, report=1):\n",
    "    load_abalone_dataset() # 데이터셋 불러오기\n",
    "    init_model() # 모델의 파라미터 초기화\n",
    "    train_and_test(epoch_count, mb_size, report) # 학습 및 평가 과정을 수행하는 부분"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_abalone_dataset():\n",
    "    with open('../../data/chap01/abalone.csv') as csvfile:\n",
    "        csvreader = csv.reader(csvfile)\n",
    "        next(csvreader, None) # next, 첫 행 날리기 / 칼럼명 부분\n",
    "        rows = []\n",
    "        for row in csvreader:\n",
    "            rows.append(row)\n",
    "            \n",
    "    global data, input_cnt, output_cnt # 전역변수 선언\n",
    "    input_cnt, output_cnt = 10, 1 # 입력벡터 크기 / 출력벡터 크기\n",
    "    data = np.zeros([len(rows), input_cnt+output_cnt]) \n",
    "    # 0으로 채운다, 오른쪽만큼\n",
    "    # 개체들의 입출력 벡터정보를 저장할 데이터 행렬 ( row수, column 수(입력 + 출력))\n",
    "\n",
    "    for n, row in enumerate(rows):\n",
    "        # one-hot vector(encoding)\n",
    "        if row[0] == 'I': data[n, 0] = 1\n",
    "        if row[0] == 'M': data[n, 1] = 1\n",
    "        if row[0] == 'F': data[n, 2] = 1\n",
    "        # 성별 외의 데이터들을 복제\n",
    "        data[n, 3:] = row[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_model():\n",
    "    global weight, bias, input_cnt, output_cnt\n",
    "    # 가중치 => 정규분포를 갖는 난숫값으로 초기화 ( np.random.normal )\n",
    "    # 파라미터의 초깃값을 실행 때마다 달라지게 하기 위함 \n",
    "    # => 매번 다른 결과를 가져와 추세를 살피며 좋은 결과를 고를 수 있도록\n",
    "    # 또한 local minimum을 피할 가능성을 높이기위함(조금이라도..)\n",
    "    weight = np.random.normal(RND_MEAN, RND_STD,[input_cnt, output_cnt])\n",
    "    # 편향 => 초기에 지나친 영향을 주어 학습에 역효과가 오지 않도록\n",
    "    # 0으로 초기화해준다.\n",
    "    bias = np.zeros([output_cnt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_test(epoch_count, mb_size, report):\n",
    "    # 데이터를 뒤섞고, 학습용 셋과 평가용 셋을 분리\n",
    "    step_count = arrange_data(mb_size)\n",
    "    # 중간, 최종 평가때 동일한 테스트 셋 사용\n",
    "    test_x, test_y = get_test_data()\n",
    "    \n",
    "    # epoch만큼 실행 ( 총 몇번 )\n",
    "    for epoch in range(epoch_count):\n",
    "        losses, accs = [], []\n",
    "        \n",
    "        # step_count만큼 실행 ( 미니배치사이즈를 횟수만큼 반복 )\n",
    "        for n in range(step_count):\n",
    "            # 학습용 미니배치 셋 획득\n",
    "            train_x, train_y = get_train_data(mb_size, n)\n",
    "            # 비용 및 정확도 획득, 적재\n",
    "            loss, acc = run_train(train_x, train_y)\n",
    "            losses.append(loss)\n",
    "            accs.append(acc)\n",
    "            \n",
    "        # 지정된 보고 주기에 따라 test 셋에서 중간 평가\n",
    "        if report > 0 and (epoch+1) % report == 0:\n",
    "            acc = run_test(test_x, test_y)\n",
    "            print('Epoch {}: loss={:5.3f}, accuracy={:5.3f}/{:5.3f}'. \\\n",
    "                  format(epoch+1, np.mean(losses), np.mean(accs), acc))\n",
    "    \n",
    "    # 최종 평가\n",
    "    final_acc = run_test(test_x, test_y)\n",
    "    print('\\nFinal Test: final accuracy = {:5.3f}'.format(final_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def arrange_data(mb_size):\n",
    "    global data, shuffle_map, test_begin_idx\n",
    "    # np.arange(n) => n까지의 수가 나열된 리스트\n",
    "    shuffle_map = np.arange(data.shape[0])\n",
    "    # 섞는다!\n",
    "    np.random.shuffle(shuffle_map)\n",
    "    # 미니배치 처리 스텝 수를 지정 \n",
    "    # => 80%만큼 학습용 데이터\n",
    "    step_count = int(data.shape[0] * 0.8) // mb_size\n",
    "    # 스텝카운트를 mb_size만큼 곱해주면 train set이 끝나고 test set이 나온다.\n",
    "    test_begin_idx = step_count * mb_size\n",
    "    return step_count\n",
    "\n",
    "def get_test_data():\n",
    "    global data, shuffle_map, test_begin_idx, output_cnt\n",
    "    # shuffle_map의 후반부( test set )\n",
    "    test_data = data[shuffle_map[test_begin_idx:]]\n",
    "    # output_cnt = 1 -> 마지막 열에 정답벡터\n",
    "    # 그 이전에까지 열들이 입력 벡터\n",
    "    return test_data[:, :-output_cnt], test_data[:, -output_cnt:]\n",
    "\n",
    "def get_train_data(mb_size, nth):\n",
    "    global data, shuffle_map, test_begin_idx, output_cnt\n",
    "    # epoch마다 다르게!!\n",
    "    # eoch마다 첫 실행에서 섞는 동작 수행\n",
    "    if nth == 0:\n",
    "        # train set 부분의 인덱스를 다시 섞는다.\n",
    "        np.random.shuffle(shuffle_map[:test_begin_idx])\n",
    "    # 해당 회차에서 다음 회차 사이의 mb_size만큼을 획득\n",
    "    train_data = data[shuffle_map[mb_size*nth:mb_size*(nth+1)]]\n",
    "    return train_data[:, :-output_cnt], train_data[:, -output_cnt:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_train(x, y):\n",
    "    # 순전파 처리\n",
    "    # 신경망 처리\n",
    "    # 입력 x로부터 신경망 출력 output 획득\n",
    "    output, aux_nn = forward_neuralnet(x)\n",
    "    # 회귀분석적 성격에 맞춘 후처리\n",
    "    # => 이진 판단, 선택 분류쪽으로 가면 이 부분이 달라진다.\n",
    "    # output과 y로부터 손실함수 loss 계산\n",
    "    loss, aux_pp = forward_postproc(output, y)\n",
    "    # aux_nn, aux_pp => 역전파에 전달하기 위한 수치\n",
    "    # 반드시 전달해야 하는 수치 && 불필요한 반복을 줄여주는 수치 ...etc\n",
    "    \n",
    "    # 보고용 정확도 계산\n",
    "    accuracy = eval_accuracy(output, y)\n",
    "    \n",
    "    # 역전파 처리\n",
    "    # 손실 기울기, 초기값은 1?\n",
    "    # 역전파의 시작!\n",
    "    G_loss = 1.0\n",
    "    # 역순이기에 후처리과정이 먼저다\n",
    "    # G_loss로부터 G_output을 얻는다. 이때, aux_pp 추가 정보를 사용\n",
    "    G_output = backprop_postproc(G_loss, aux_pp)\n",
    "    # 원칙적으로는, G_output으로부터 G_x를 구하는 것\n",
    "    # x는 딥러닝 알고리즘으로 손댈 수 없는 고정값이면서\n",
    "    # 더이상 수행될 역전파도 없으니 따로 반환을 할 필요는 없다.\n",
    "    # ** 중요한 점 **\n",
    "    # backprop_neuralnet()가 실행되는 중에,\n",
    "    # 신경망 파라미터값의 변화, 학습이 실제로 일어난다.\n",
    "    backprop_neuralnet(G_output, aux_nn)\n",
    "    \n",
    "    return loss, accuracy\n",
    "\n",
    "def run_test(x, y):\n",
    "    # 역전파 전달 데이터가 필요없으니 _로 처리 (dummy)\n",
    "    # 후처리 또한 필요없다!!\n",
    "    output, _ = forward_neuralnet(x)\n",
    "    accuracy = eval_accuracy(output, y)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_neuralnet(x):\n",
    "    global weight, bias\n",
    "    # 입력 행렬 x에 가중치 행렬 weight를 곱하고,\n",
    "    # 편향 벡터 bias를 더하면 끝\n",
    "    output = np.matmul(x, weight) + bias\n",
    "    # x => [N, 10] * weight = [10, 1] = [N, 1]\n",
    "    # [N, 1] + [1] => 각 행에 합연산 적용\n",
    "    return output, x\n",
    "\n",
    "def backprop_neuralnet(G_output, x):\n",
    "    global weight, bias\n",
    "    # np.transpose() => 행, 열 바꾸기\n",
    "    g_output_w = x.transpose() # [10, N]\n",
    "    \n",
    "    # weight, bias의 손실기울기 계산\n",
    "    G_w = np.matmul(g_output_w, G_output)# [10, N] * [N, 1] = [10, 1]\n",
    "    G_b = np.sum(G_output, axis=0) # [N,1]의 각 행 값의 합 => [1]\n",
    "    \n",
    "    # 실제 값에 적용\n",
    "    weight -= LEARNING_RATE * G_w\n",
    "    bias -= LEARNING_RATE * G_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_postproc(output, y):\n",
    "    # 손실함수, 평균제곱오차값 => MSE( Mean Square Error )\n",
    "    # 신경망 출력 - 정답행렬 => [N,1]\n",
    "    diff = output - y \n",
    "    # 각 수치들에 제곱 => [N,1]\n",
    "    square = np.square(diff)\n",
    "    # 각 수치들의 평균 => [1]\n",
    "    loss = np.mean(square)\n",
    "    return loss, diff\n",
    "\n",
    "def backprop_postproc(G_loss, diff):\n",
    "    shape = diff.shape\n",
    "    \n",
    "    # 아래는 수학의 영역... p.82\n",
    "    # 입출력간 부분의 기울기\n",
    "    g_loss_square = np.ones(shape) / np.prod(shape)\n",
    "    g_square_diff = 2 * diff\n",
    "    g_diff_output = 1\n",
    "    \n",
    "    # 평균, 제곱, 오차 연산에 대한 역전파 처리\n",
    "    # G_loss로부터 G_output을 얻어내게 된다.\n",
    "    G_square = g_loss_square * G_loss\n",
    "    G_diff = g_square_diff * G_square\n",
    "    G_output = g_diff_output * G_diff\n",
    "    \n",
    "    return G_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_accuracy(output, y):\n",
    "    # 다양한 방법들이 존재.\n",
    "    # 여기서는 정답과 오차의 비율을 오류율로 본다.\n",
    "    mdiff = np.mean(np.abs((output - y)/y))\n",
    "    return 1 - mdiff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 정리된 방식 => 위에서는 흐름 전체를 나열\n",
    "def backprop_postproc_oneline(G_loss, diff):  # backprop_postproc() 대신 사용 가능\n",
    "    return 2 * diff / np.prod(diff.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
